# Optional: Run Ollama as a Docker service
# Usage:
#   Development: docker compose -f docker-compose.dev.yml -f docker-compose.ollama.yml up
#   Production:  docker compose -f docker-compose.yml -f docker-compose.ollama.yml up --build

services:
  # Extend redis to be on the same network
  redis:
    networks:
      - relevel-network

  # Extend web service to wait for Ollama
  web:
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - relevel-network

  # Extend worker service to wait for Ollama
  worker:
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - relevel-network

  ollama:
    build:
      context: .
      dockerfile: Dockerfile.ollama
      args:
        - OLLAMA_MODEL=llama3.2:latest  # Model to bake into the image
    container_name: relevel-ollama
    ports:
      - "11434:11434"
    networks:
      - relevel-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Uncomment for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

networks:
  relevel-network:
    driver: bridge
